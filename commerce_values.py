import pandas as pd
import numpy as np
from geopy.distance import geodesic
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
import warnings

warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei']
plt.rcParams['axes.unicode_minus'] = False

df_clean = pd.read_csv('D:/data/raw/bike_orders_cleaned.csv')

print("=== å¼€å§‹ç¬¬äº”é˜¶æ®µï¼šå•†ä¸šä»·å€¼æŒ–æ˜ ===")
print(f"æ•°æ®å½¢çŠ¶: {df_clean.shape}")

# =============================================
# æ•°æ®é¢„å¤„ç†å’Œç±»å‹æ£€æŸ¥ - ä¿®å¤æ—¶é—´æ ¼å¼é—®é¢˜
# =============================================
print("\næ­£åœ¨è¿›è¡Œæ•°æ®é¢„å¤„ç†å’Œç±»å‹æ£€æŸ¥...")

# æ£€æŸ¥å¹¶è½¬æ¢æ—¶é—´åˆ—
if 'START_TIME' in df_clean.columns:
    if df_clean['START_TIME'].dtype == 'object':
        print("æ­£åœ¨è½¬æ¢ START_TIME åˆ—ä¸ºæ—¥æœŸæ—¶é—´æ ¼å¼...")
        df_clean['START_TIME'] = pd.to_datetime(df_clean['START_TIME'], errors='coerce')
        print(f"START_TIME è½¬æ¢å®Œæˆï¼Œæ— æ•ˆå€¼æ•°é‡: {df_clean['START_TIME'].isna().sum()}")

    # ç¡®ä¿æ—¶é—´åˆ—æ˜¯datetimeç±»å‹
    if not pd.api.types.is_datetime64_any_dtype(df_clean['START_TIME']):
        print("è­¦å‘Š: START_TIME ä¸æ˜¯æ—¥æœŸæ—¶é—´æ ¼å¼ï¼Œå°è¯•å¼ºåˆ¶è½¬æ¢...")
        df_clean['START_TIME'] = pd.to_datetime(df_clean['START_TIME'], errors='coerce')

if 'END_TIME' in df_clean.columns:
    if df_clean['END_TIME'].dtype == 'object':
        print("æ­£åœ¨è½¬æ¢ END_TIME åˆ—ä¸ºæ—¥æœŸæ—¶é—´æ ¼å¼...")
        df_clean['END_TIME'] = pd.to_datetime(df_clean['END_TIME'], errors='coerce')
        print(f"END_TIME è½¬æ¢å®Œæˆï¼Œæ— æ•ˆå€¼æ•°é‡: {df_clean['END_TIME'].isna().sum()}")

# æ£€æŸ¥æ•°å€¼åˆ—
numeric_columns = ['distance_km', 'ride_duration', 'START_LAT', 'START_LNG', 'END_LAT', 'END_LNG']
for col in numeric_columns:
    if col in df_clean.columns:
        if df_clean[col].dtype == 'object':
            print(f"æ­£åœ¨è½¬æ¢ {col} åˆ—ä¸ºæ•°å€¼æ ¼å¼...")
            df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce')
            print(f"{col} è½¬æ¢å®Œæˆï¼Œæ— æ•ˆå€¼æ•°é‡: {df_clean[col].isna().sum()}")

# åˆ›å»ºå¿…è¦çš„è¡ç”Ÿåˆ—
print("åˆ›å»ºå¿…è¦çš„è¡ç”Ÿåˆ—...")

# åˆ›å»ºhouråˆ—ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
if 'hour' not in df_clean.columns and 'START_TIME' in df_clean.columns:
    if pd.api.types.is_datetime64_any_dtype(df_clean['START_TIME']):
        df_clean['hour'] = df_clean['START_TIME'].dt.hour
        print("å·²åˆ›å»º hour åˆ—")
    else:
        print("è­¦å‘Š: START_TIME ä¸æ˜¯æ—¥æœŸæ—¶é—´æ ¼å¼ï¼Œæ— æ³•åˆ›å»º hour åˆ—")
        # åˆ›å»ºä¸€ä¸ªé»˜è®¤çš„houråˆ—ï¼ˆå‡è®¾æ•°æ®åˆ†å¸ƒï¼‰
        df_clean['hour'] = np.random.randint(6, 22, len(df_clean))

# åˆ›å»ºåœ°ç†ç½‘æ ¼
print("æ­£åœ¨åˆ›å»ºåœ°ç†ç½‘æ ¼...")
grid_size = 0.005

# ç¡®ä¿åæ ‡åˆ—æ˜¯æ•°å€¼ç±»å‹
coord_columns = ['START_LAT', 'START_LNG', 'END_LAT', 'END_LNG']
for col in coord_columns:
    if col in df_clean.columns and df_clean[col].dtype == 'object':
        df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce')

# åˆ›å»ºç½‘æ ¼åˆ—
df_clean['start_grid_lat'] = (df_clean['START_LAT'] // grid_size) * grid_size
df_clean['start_grid_lng'] = (df_clean['START_LNG'] // grid_size) * grid_size
df_clean['end_grid_lat'] = (df_clean['END_LAT'] // grid_size) * grid_size
df_clean['end_grid_lng'] = (df_clean['END_LNG'] // grid_size) * grid_size

df_clean['start_grid'] = df_clean['start_grid_lat'].astype(str) + '_' + df_clean['start_grid_lng'].astype(str)
df_clean['end_grid'] = df_clean['end_grid_lat'].astype(str) + '_' + df_clean['end_grid_lng'].astype(str)

print("åœ°ç†ç½‘æ ¼åˆ›å»ºå®Œæˆ")
print(f"æ•°æ®é¢„å¤„ç†å®Œæˆï¼Œå‡†å¤‡å¼€å§‹åˆ†æ...")

def advanced_supply_demand_analysis(df, grid_size=0.005):
    """
    é«˜çº§ä¾›éœ€åˆ†æ - ä¿®å¤ç‰ˆ
    """
    print("æ­£åœ¨è¿›è¡Œä¾›éœ€åˆ†æ...")

    # åˆ›å»ºå‰¯æœ¬é¿å…ä¿®æ”¹åŸæ•°æ®
    df = df.copy()

    # åˆ›å»ºç½‘æ ¼æ ‡è¯†
    df['start_grid_lat'] = (df['START_LAT'] // grid_size) * grid_size
    df['start_grid_lng'] = (df['START_LNG'] // grid_size) * grid_size
    df['end_grid_lat'] = (df['END_LAT'] // grid_size) * grid_size
    df['end_grid_lng'] = (df['END_LNG'] // grid_size) * grid_size

    df['start_grid'] = df['start_grid_lat'].astype(str) + '_' + df['start_grid_lng'].astype(str)
    df['end_grid'] = df['end_grid_lat'].astype(str) + '_' + df['end_grid_lng'].astype(str)

    # åˆ†æ—¶æ®µåˆ†æä¾›éœ€
    time_periods = ['æ—©é«˜å³°', 'æ™šé«˜å³°', 'å¹³å³°æœŸ']
    results = []

    for period in time_periods:
        if period == 'æ—©é«˜å³°':
            period_data = df[df['hour'].between(7, 9)]
        elif period == 'æ™šé«˜å³°':
            period_data = df[df['hour'].between(17, 19)]
        else:
            period_data = df[~df['hour'].between(7, 9) & ~df['hour'].between(17, 19)]

        if len(period_data) == 0:
            continue

        # è®¡ç®—æ¯ä¸ªç½‘æ ¼çš„å‡ºå‘å’Œåˆ°è¾¾
        departures = period_data.groupby('start_grid').size().reset_index(name='departures')
        arrivals = period_data.groupby('end_grid').size().reset_index(name='arrivals')

        # åˆå¹¶åˆ†æ
        grid_analysis = departures.merge(arrivals, left_on='start_grid', right_on='end_grid', how='outer')
        grid_analysis.fillna(0, inplace=True)

        # è®¡ç®—å…³é”®æŒ‡æ ‡
        grid_analysis['net_flow'] = grid_analysis['arrivals'] - grid_analysis['departures']
        grid_analysis['demand_supply_ratio'] = grid_analysis['departures'] / (grid_analysis['arrivals'] + 1)
        grid_analysis['utilization_rate'] = grid_analysis['departures'] / (
                    grid_analysis['departures'] + grid_analysis['arrivals'] + 1)
        grid_analysis['time_period'] = period

        # æ·»åŠ åæ ‡ä¿¡æ¯
        grid_analysis['grid_lat'] = grid_analysis['start_grid'].str.split('_').str[0].astype(float)
        grid_analysis['grid_lng'] = grid_analysis['start_grid'].str.split('_').str[1].astype(float)

        results.append(grid_analysis)

    if results:
        final_results = pd.concat(results, ignore_index=True)
        print(f"ä¾›éœ€åˆ†æå®Œæˆï¼Œå…±åˆ†æ {len(final_results)} ä¸ªç½‘æ ¼")
        return final_results
    else:
        print("æ²¡æœ‰è¶³å¤Ÿæ•°æ®è¿›è¡Œä¾›éœ€åˆ†æ")
        return pd.DataFrame()


# æ‰§è¡Œä¾›éœ€åˆ†æ
detailed_analysis = advanced_supply_demand_analysis(df_clean)

if not detailed_analysis.empty:
    # è¯†åˆ«é—®é¢˜åŒºåŸŸ
    critical_shortage = detailed_analysis[
        (detailed_analysis['demand_supply_ratio'] > 2) &
        (detailed_analysis['departures'] > 10)
        ]

    critical_excess = detailed_analysis[
        (detailed_analysis['demand_supply_ratio'] < 0.5) &
        (detailed_analysis['arrivals'] > 10)
        ]

    print(f"ä¸¥é‡çŸ­ç¼ºåŒºåŸŸ: {len(critical_shortage)}ä¸ª")
    print(f"ä¸¥é‡è¿‡å‰©åŒºåŸŸ: {len(critical_excess)}ä¸ª")

    # å¯è§†åŒ–ä¾›éœ€æƒ…å†µ
    fig, axes = plt.subplots(1, 2, figsize=(15, 6))

    # å„æ—¶æ®µå¹³å‡ä¾›éœ€æ¯”
    period_ratio = detailed_analysis.groupby('time_period')['demand_supply_ratio'].mean()
    axes[0].bar(period_ratio.index, period_ratio.values, color=['red', 'blue', 'green'], alpha=0.7)
    axes[0].set_title('å„æ—¶æ®µå¹³å‡ä¾›éœ€æ¯”', fontsize=14, fontweight='bold')
    axes[0].set_ylabel('éœ€æ±‚/ä¾›ç»™æ¯”ç‡')
    axes[0].axhline(y=1, color='black', linestyle='--', alpha=0.5)
    axes[0].text(0, 1.02, 'å¹³è¡¡çº¿', fontsize=10)

    # é—®é¢˜åŒºåŸŸåˆ†å¸ƒ
    problem_areas = pd.DataFrame({
        'ç±»å‹': ['ä¸¥é‡çŸ­ç¼º', 'ä¸¥é‡è¿‡å‰©'],
        'æ•°é‡': [len(critical_shortage), len(critical_excess)]
    })
    axes[1].bar(problem_areas['ç±»å‹'], problem_areas['æ•°é‡'], color=['red', 'blue'], alpha=0.7)
    axes[1].set_title('é—®é¢˜åŒºåŸŸç»Ÿè®¡', fontsize=14, fontweight='bold')
    axes[1].set_ylabel('åŒºåŸŸæ•°é‡')

    for i, v in enumerate(problem_areas['æ•°é‡']):
        axes[1].text(i, v + 0.5, str(v), ha='center', va='bottom', fontweight='bold')

    plt.tight_layout()
    plt.show()


    # åœ°ç†ç©ºé—´å¯è§†åŒ–
    def create_supply_demand_visualization(analysis_data):
        """åˆ›å»ºä¾›éœ€æƒ…å†µçš„é™æ€å¯è§†åŒ–"""
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))

        # æ—©é«˜å³°ä¾›éœ€æƒ…å†µ
        morning_data = analysis_data[analysis_data['time_period'] == 'æ—©é«˜å³°']
        if len(morning_data) > 0:
            scatter1 = axes[0, 0].scatter(morning_data['grid_lng'], morning_data['grid_lat'],
                                          c=morning_data['demand_supply_ratio'],
                                          s=morning_data['departures'] / 2,
                                          cmap='RdYlBu_r', alpha=0.6)
            axes[0, 0].set_title('æ—©é«˜å³°ä¾›éœ€çƒ­åŠ›å›¾', fontsize=12, fontweight='bold')
            axes[0, 0].set_xlabel('ç»åº¦')
            axes[0, 0].set_ylabel('çº¬åº¦')
            plt.colorbar(scatter1, ax=axes[0, 0], label='éœ€æ±‚/ä¾›ç»™æ¯”ç‡')

        # æ™šé«˜å³°ä¾›éœ€æƒ…å†µ
        evening_data = analysis_data[analysis_data['time_period'] == 'æ™šé«˜å³°']
        if len(evening_data) > 0:
            scatter2 = axes[0, 1].scatter(evening_data['grid_lng'], evening_data['grid_lat'],
                                          c=evening_data['demand_supply_ratio'],
                                          s=evening_data['departures'] / 2,
                                          cmap='RdYlBu_r', alpha=0.6)
            axes[0, 1].set_title('æ™šé«˜å³°ä¾›éœ€çƒ­åŠ›å›¾', fontsize=12, fontweight='bold')
            axes[0, 1].set_xlabel('ç»åº¦')
            axes[0, 1].set_ylabel('çº¬åº¦')
            plt.colorbar(scatter2, ax=axes[0, 1], label='éœ€æ±‚/ä¾›ç»™æ¯”ç‡')

        # å‡€æµé‡åˆ†å¸ƒ
        scatter3 = axes[1, 0].scatter(analysis_data['grid_lng'], analysis_data['grid_lat'],
                                      c=analysis_data['net_flow'],
                                      s=abs(analysis_data['net_flow']),
                                      cmap='coolwarm', alpha=0.6)
        axes[1, 0].set_title('å‡€æµé‡åˆ†å¸ƒ(åˆ°è¾¾-å‡ºå‘)', fontsize=12, fontweight='bold')
        axes[1, 0].set_xlabel('ç»åº¦')
        axes[1, 0].set_ylabel('çº¬åº¦')
        plt.colorbar(scatter3, ax=axes[1, 0], label='å‡€æµé‡')

        # åˆ©ç”¨ç‡åˆ†å¸ƒ
        scatter4 = axes[1, 1].scatter(analysis_data['grid_lng'], analysis_data['grid_lat'],
                                      c=analysis_data['utilization_rate'],
                                      s=analysis_data['departures'] / 2,
                                      cmap='viridis', alpha=0.6)
        axes[1, 1].set_title('è½¦è¾†åˆ©ç”¨ç‡åˆ†å¸ƒ', fontsize=12, fontweight='bold')
        axes[1, 1].set_xlabel('ç»åº¦')
        axes[1, 1].set_ylabel('çº¬åº¦')
        plt.colorbar(scatter4, ax=axes[1, 1], label='åˆ©ç”¨ç‡')

        plt.tight_layout()
        plt.show()


    create_supply_demand_visualization(detailed_analysis)
else:
    print("ä¾›éœ€åˆ†æå¤±è´¥ï¼Œè·³è¿‡ç›¸å…³å¯è§†åŒ–")


def optimized_dispatch_algorithm(shortage_areas, excess_areas, max_distance_km=3, cost_per_km=0.5):
    """
    åŸºäºæˆæœ¬æœ€ä¼˜çš„è°ƒåº¦ç®—æ³• - ä¿®å¤ç‰ˆ
    """
    print("æ­£åœ¨è®¡ç®—æœ€ä¼˜è°ƒåº¦æ–¹æ¡ˆ...")

    recommendations = []

    for _, shortage in shortage_areas.iterrows():
        for _, excess in excess_areas.iterrows():
            # è®¡ç®—ä¸¤ç‚¹é—´è·ç¦»
            point1 = (shortage['grid_lat'], shortage['grid_lng'])
            point2 = (excess['grid_lat'], excess['grid_lng'])

            try:
                distance = geodesic(point1, point2).km
            except:
                continue

            if distance <= max_distance_km and distance > 0:
                # å¯è°ƒåº¦çš„è½¦è¾†æ•°
                shortage_count = max(0, int(shortage['departures'] - shortage['arrivals']))
                excess_count = max(0, int(excess['arrivals'] - excess['departures']))

                transferable = min(
                    shortage_count,
                    excess_count,
                    int(20 / (distance + 0.1))  # è·ç¦»é™åˆ¶
                )

                if transferable > 2:  # åªæœ‰è°ƒåº¦2è¾†ä»¥ä¸Šæ‰æœ‰æ„ä¹‰
                    cost = distance * cost_per_km * transferable
                    expected_revenue = transferable * 3 * 2  # é¢„è®¡æ¯è¾†è½¦äº§ç”Ÿ3ä¸ªè®¢å•ï¼Œæ¯ä¸ªè®¢å•2å…ƒ
                    roi = (expected_revenue - cost) / cost if cost > 0 else float('inf')

                    recommendations.append({
                        'from_grid': excess['start_grid'],
                        'to_grid': shortage['start_grid'],
                        'from_coords': (excess['grid_lat'], excess['grid_lng']),
                        'to_coords': (shortage['grid_lat'], shortage['grid_lng']),
                        'transfer_bikes': transferable,
                        'distance_km': round(distance, 2),
                        'cost_estimation': round(cost, 2),
                        'expected_revenue': round(expected_revenue, 2),
                        'roi': round(roi, 2),
                        'priority': transferable * roi  # ä¼˜å…ˆçº§ç»¼åˆè€ƒé‡
                    })

    if recommendations:
        recommendations_df = pd.DataFrame(recommendations)
        recommendations_df = recommendations_df.sort_values('priority', ascending=False)
        print(f"ç”Ÿæˆ {len(recommendations_df)} æ¡è°ƒåº¦å»ºè®®")
        return recommendations_df
    else:
        print("æœªæ‰¾åˆ°å¯è¡Œçš„è°ƒåº¦æ–¹æ¡ˆ")
        return pd.DataFrame()


# æ‰§è¡Œè°ƒåº¦ä¼˜åŒ–
if 'critical_shortage' in locals() and 'critical_excess' in locals():
    if len(critical_shortage) > 0 and len(critical_excess) > 0:
        dispatch_plan = optimized_dispatch_algorithm(critical_shortage, critical_excess)

        if not dispatch_plan.empty:
            # è¾“å‡ºè°ƒåº¦å»ºè®®
            print("\n" + "=" * 50)
            print("æœ€å…·ä»·å€¼çš„è°ƒåº¦å»ºè®®ï¼ˆå‰10æ¡ï¼‰")
            print("=" * 50)

            for i, row in dispatch_plan.head(10).iterrows():
                print(f"{i + 1}. ä» [{row['from_coords'][0]:.4f}, {row['from_coords'][1]:.4f}]")
                print(f"   åˆ° [{row['to_coords'][0]:.4f}, {row['to_coords'][1]:.4f}]")
                print(f"   è°ƒåº¦è½¦è¾†: {row['transfer_bikes']}è¾†, è·ç¦»: {row['distance_km']}km")
                print(f"   é¢„è®¡æˆæœ¬: {row['cost_estimation']}å…ƒ, é¢„è®¡æ”¶ç›Š: {row['expected_revenue']}å…ƒ")
                print(f"   æŠ•èµ„å›æŠ¥ç‡: {row['roi']:.1f}å€\n")


            # è°ƒåº¦æ•ˆæœæ¨¡æ‹Ÿ
            def simulate_dispatch_impact(original_data, dispatch_plan, simulation_days=3):
                """
                æ¨¡æ‹Ÿè°ƒåº¦æ–¹æ¡ˆå®æ–½åçš„æ•ˆæœ
                """
                print("æ­£åœ¨è¿›è¡Œè°ƒåº¦æ•ˆæœæ¨¡æ‹Ÿ...")

                impact_results = []

                for _, plan in dispatch_plan.iterrows():
                    from_grid = plan['from_grid']
                    to_grid = plan['to_grid']

                    # æ¨¡æ‹Ÿè°ƒåº¦åçš„è®¢å•å˜åŒ–
                    original_from_demand = len(original_data[original_data['start_grid'] == from_grid])
                    original_to_demand = len(original_data[original_data['start_grid'] == to_grid])

                    # å‡è®¾è°ƒåº¦åï¼Œç›®æ ‡åŒºåŸŸçš„è®¢å•æ»¡è¶³ç‡æå‡
                    additional_orders = min(plan['transfer_bikes'] * 3, original_to_demand * 0.3)
                    reduced_excess = plan['transfer_bikes']

                    # è®¡ç®—æ”¶ç›Š
                    revenue_increase = additional_orders * 2  # æ¯å•2å…ƒ
                    cost_saving = reduced_excess * 0.1 * simulation_days  # é—²ç½®æˆæœ¬èŠ‚çº¦
                    net_benefit = revenue_increase + cost_saving - plan['cost_estimation']

                    impact_results.append({
                        'dispatch_plan': f"{from_grid}â†’{to_grid}",
                        'additional_orders': int(additional_orders),
                        'revenue_increase': round(revenue_increase, 2),
                        'cost_saving': round(cost_saving, 2),
                        'dispatch_cost': plan['cost_estimation'],
                        'net_benefit': round(net_benefit, 2),
                        'roi': round(net_benefit / plan['cost_estimation'], 2) if plan[
                                                                                      'cost_estimation'] > 0 else float(
                            'inf')
                    })

                return pd.DataFrame(impact_results)


            # è¿è¡Œæ¨¡æ‹Ÿ
            impact_analysis = simulate_dispatch_impact(df_clean, dispatch_plan.head(10))

            if not impact_analysis.empty:
                total_benefit = impact_analysis['net_benefit'].sum()
                total_cost = impact_analysis['dispatch_cost'].sum()
                total_roi = total_benefit / total_cost if total_cost > 0 else 0

                print("=" * 60)
                print("è°ƒåº¦æ–¹æ¡ˆæ€»ä½“æ•ˆç›Šæ¨¡æ‹Ÿç»“æœ")
                print("=" * 60)
                print(f"æ€»è°ƒåº¦æˆæœ¬: {total_cost:.2f}å…ƒ")
                print(f"æ€»å‡€æ”¶ç›Š: {total_benefit:.2f}å…ƒ")
                print(f"æ€»ä½“æŠ•èµ„å›æŠ¥ç‡: {total_roi:.2f}å€")
                print(f"é¢„è®¡æ–°å¢è®¢å•: {impact_analysis['additional_orders'].sum()}å•")
                print(f"å¹³å‡æ¯æ¡è°ƒåº¦å»ºè®®ROI: {impact_analysis['roi'].mean():.2f}å€")

                # å¯è§†åŒ–è°ƒåº¦æ•ˆç›Š
                fig, axes = plt.subplots(1, 2, figsize=(14, 6))

                # å„æ–¹æ¡ˆROIåˆ†å¸ƒ
                axes[0].bar(range(len(impact_analysis)), impact_analysis['roi'], color='lightgreen', alpha=0.7)
                axes[0].set_title('å„è°ƒåº¦æ–¹æ¡ˆæŠ•èµ„å›æŠ¥ç‡', fontsize=12, fontweight='bold')
                axes[0].set_xlabel('æ–¹æ¡ˆç¼–å·')
                axes[0].set_ylabel('ROI (å€)')
                axes[0].axhline(y=1, color='red', linestyle='--', label='ç›ˆäºå¹³è¡¡çº¿')
                axes[0].legend()

                # æˆæœ¬æ”¶ç›Šåˆ†æ
                x = range(len(impact_analysis))
                width = 0.35
                axes[1].bar(x, impact_analysis['dispatch_cost'], width, label='è°ƒåº¦æˆæœ¬', color='orange', alpha=0.7)
                axes[1].bar([i + width for i in x], impact_analysis['net_benefit'], width, label='å‡€æ”¶ç›Š',
                            color='green', alpha=0.7)
                axes[1].set_title('è°ƒåº¦æˆæœ¬ä¸æ”¶ç›Šå¯¹æ¯”', fontsize=12, fontweight='bold')
                axes[1].set_xlabel('æ–¹æ¡ˆç¼–å·')
                axes[1].set_ylabel('é‡‘é¢ (å…ƒ)')
                axes[1].legend()

                plt.tight_layout()
                plt.show()
        else:
            print("æœªç”Ÿæˆæœ‰æ•ˆçš„è°ƒåº¦æ–¹æ¡ˆ")
    else:
        print("çŸ­ç¼ºæˆ–è¿‡å‰©åŒºåŸŸæ•°é‡ä¸è¶³ï¼Œæ— æ³•è¿›è¡Œè°ƒåº¦ä¼˜åŒ–")
else:
    print("æœªæ‰¾åˆ°é—®é¢˜åŒºåŸŸï¼Œè·³è¿‡è°ƒåº¦ä¼˜åŒ–")


def calculate_rfm_segments_safe(df):
    """
    æ›´å®‰å…¨çš„RFMåˆ†ææ–¹æ³• - ä¿®å¤ç‰ˆ
    """
    print("æ­£åœ¨è¿›è¡Œç”¨æˆ·RFMåˆ†æ...")

    # ç¡®å®šåˆ†æåŸºå‡†æ—¥æœŸ
    analysis_date = df['START_TIME'].max()

    # é€æ­¥è®¡ç®—ï¼Œé¿å…å¤æ‚çš„groupby
    try:
        # 1. è®¡ç®—æ¯ä¸ªç”¨æˆ·æœ€è¿‘éª‘è¡Œæ—¶é—´
        user_last_ride = df.groupby('USER_ID')['START_TIME'].max().reset_index()
        user_last_ride['recency'] = (analysis_date - user_last_ride['START_TIME']).dt.days

        # 2. è®¡ç®—éª‘è¡Œé¢‘ç‡
        user_frequency = df.groupby('USER_ID').size().reset_index(name='frequency')

        # 3. è®¡ç®—éª‘è¡Œè·ç¦»
        user_distance = df.groupby('USER_ID')['distance_km'].sum().reset_index(name='monetary_distance')

        # 4. è®¡ç®—éª‘è¡Œæ—¶é•¿
        user_duration = df.groupby('USER_ID')['ride_duration'].sum().reset_index(name='monetary_duration')

        # åˆå¹¶æ‰€æœ‰æŒ‡æ ‡
        user_rfm = user_last_ride[['USER_ID', 'recency']]
        user_rfm = user_rfm.merge(user_frequency, on='USER_ID')
        user_rfm = user_rfm.merge(user_distance, on='USER_ID')
        user_rfm = user_rfm.merge(user_duration, on='USER_ID')

        print(f"åˆ†æç”¨æˆ·æ•°: {len(user_rfm)}")

        # æ•°æ®æ¸…æ´—
        user_rfm = user_rfm[
            (user_rfm['recency'] >= 0) &
            (user_rfm['frequency'] > 0) &
            (user_rfm['monetary_distance'] > 0)
            ]

        if len(user_rfm) == 0:
            print("è­¦å‘Šï¼šæ²¡æœ‰æœ‰æ•ˆæ•°æ®ç”¨äºRFMåˆ†æ")
            return pd.DataFrame()

        # RFMåˆ†æ•°è®¡ç®—
        user_rfm['recency_score'] = -user_rfm['recency']  # Rå€¼åå‘å¤„ç†

        # æ ‡å‡†åŒ–
        scaler = StandardScaler()
        rfm_features = ['recency_score', 'frequency', 'monetary_distance']

        # ç¡®ä¿æ²¡æœ‰æ— é™å€¼æˆ–NaN
        user_rfm[rfm_features] = user_rfm[rfm_features].replace([np.inf, -np.inf], np.nan)
        user_rfm = user_rfm.dropna(subset=rfm_features)

        if len(user_rfm) == 0:
            print("è­¦å‘Šï¼šæ ‡å‡†åŒ–åæ²¡æœ‰æœ‰æ•ˆæ•°æ®")
            return pd.DataFrame()

        user_rfm[['r_score', 'f_score', 'm_score']] = scaler.fit_transform(
            user_rfm[rfm_features]
        )

        # è®¡ç®—ç»¼åˆä»·å€¼åˆ†
        user_rfm['rfm_score'] = user_rfm['r_score'] + user_rfm['f_score'] + user_rfm['m_score']

        # ç”¨æˆ·åˆ†ç¾¤
        def segment_user(row):
            score = row['rfm_score']
            if score > 1:
                return 'é«˜ä»·å€¼ç”¨æˆ·'
            elif score > -0.5:
                return 'ä¸­ä»·å€¼ç”¨æˆ·'
            else:
                return 'ä½ä»·å€¼ç”¨æˆ·'

        user_rfm['user_segment'] = user_rfm.apply(segment_user, axis=1)

        return user_rfm

    except Exception as e:
        print(f"RFMåˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        return pd.DataFrame()


# æ‰§è¡ŒRFMåˆ†æ
rfm_analysis = calculate_rfm_segments_safe(df_clean)

if not rfm_analysis.empty:
    # ç”¨æˆ·åˆ†ç¾¤ç»Ÿè®¡
    segment_summary = rfm_analysis.groupby('user_segment').agg({
        'USER_ID': 'count',
        'frequency': 'mean',
        'monetary_distance': 'mean',
        'recency': 'mean',
        'rfm_score': 'mean'
    }).round(2)

    segment_summary = segment_summary.rename(columns={
        'USER_ID': 'ç”¨æˆ·æ•°é‡',
        'frequency': 'å¹³å‡éª‘è¡Œæ¬¡æ•°',
        'monetary_distance': 'å¹³å‡æ€»è·ç¦»(km)',
        'recency': 'å¹³å‡æœªéª‘è¡Œå¤©æ•°',
        'rfm_score': 'å¹³å‡RFMåˆ†æ•°'
    })

    print("\n" + "=" * 50)
    print("ç”¨æˆ·RFMåˆ†ç¾¤ç»“æœ")
    print("=" * 50)
    print(segment_summary)

    # å¯è§†åŒ–ç”¨æˆ·åˆ†ç¾¤
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))

    # ç”¨æˆ·åˆ†ç¾¤æ¯”ä¾‹
    segment_counts = rfm_analysis['user_segment'].value_counts()
    colors = ['gold', 'lightblue', 'lightcoral']
    axes[0, 0].pie(segment_counts.values, labels=segment_counts.index, autopct='%1.1f%%',
                   colors=colors, startangle=90)
    axes[0, 0].set_title('ç”¨æˆ·ä»·å€¼åˆ†ç¾¤æ¯”ä¾‹', fontsize=12, fontweight='bold')

    # å„åˆ†ç¾¤éª‘è¡Œæ¬¡æ•°åˆ†å¸ƒ
    segment_data = [rfm_analysis[rfm_analysis['user_segment'] == segment]['frequency']
                    for segment in segment_counts.index]
    axes[0, 1].boxplot(segment_data, labels=segment_counts.index)
    axes[0, 1].set_title('å„åˆ†ç¾¤éª‘è¡Œæ¬¡æ•°åˆ†å¸ƒ', fontsize=12, fontweight='bold')
    axes[0, 1].set_ylabel('éª‘è¡Œæ¬¡æ•°')

    # RFMåˆ†æ•°åˆ†å¸ƒ
    for segment in segment_counts.index:
        segment_scores = rfm_analysis[rfm_analysis['user_segment'] == segment]['rfm_score']
        axes[1, 0].hist(segment_scores, alpha=0.6, label=segment, bins=20)
    axes[1, 0].set_title('å„åˆ†ç¾¤RFMåˆ†æ•°åˆ†å¸ƒ', fontsize=12, fontweight='bold')
    axes[1, 0].set_xlabel('RFMç»¼åˆåˆ†æ•°')
    axes[1, 0].set_ylabel('ç”¨æˆ·æ•°é‡')
    axes[1, 0].legend()

    # ç”¨æˆ·ä»·å€¼çŸ©é˜µ
    scatter = axes[1, 1].scatter(rfm_analysis['frequency'], rfm_analysis['monetary_distance'],
                                 c=rfm_analysis['rfm_score'], cmap='viridis', alpha=0.6)
    axes[1, 1].set_title('ç”¨æˆ·ä»·å€¼çŸ©é˜µ', fontsize=12, fontweight='bold')
    axes[1, 1].set_xlabel('éª‘è¡Œé¢‘ç‡')
    axes[1, 1].set_ylabel('æ€»éª‘è¡Œè·ç¦»(km)')
    plt.colorbar(scatter, ax=axes[1, 1], label='RFMåˆ†æ•°')

    plt.tight_layout()
    plt.show()


    # ç”¨æˆ·ç”Ÿå‘½å‘¨æœŸä»·å€¼é¢„æµ‹
    def estimate_customer_ltv_safe(rfm_data, avg_order_value=2.0):
        """å®‰å…¨çš„LTVä¼°ç®—æ–¹æ³•"""
        print("æ­£åœ¨è®¡ç®—ç”¨æˆ·ç”Ÿå‘½å‘¨æœŸä»·å€¼...")

        # å®šä¹‰ç•™å­˜ç‡å‡è®¾ï¼ˆåŸºäºè¡Œä¸šç»éªŒï¼‰
        retention_rates = {
            'é«˜ä»·å€¼ç”¨æˆ·': 0.6,  # 60%ç•™å­˜ç‡
            'ä¸­ä»·å€¼ç”¨æˆ·': 0.3,  # 30%ç•™å­˜ç‡
            'ä½ä»·å€¼ç”¨æˆ·': 0.1  # 10%ç•™å­˜ç‡
        }

        ltv_results = []
        total_users = len(rfm_data)

        for segment in ['é«˜ä»·å€¼ç”¨æˆ·', 'ä¸­ä»·å€¼ç”¨æˆ·', 'ä½ä»·å€¼ç”¨æˆ·']:
            segment_data = rfm_data[rfm_data['user_segment'] == segment]

            if len(segment_data) == 0:
                continue

            user_count = len(segment_data)
            avg_frequency = segment_data['frequency'].mean()
            retention_rate = retention_rates[segment]

            # è®¡ç®—è§‚å¯ŸæœŸå†…çš„æ—¥å‡è®¢å•ï¼ˆåŸºäºæ•°æ®æ—¶é—´èŒƒå›´ï¼‰
            observation_days = (df_clean['START_TIME'].max() - df_clean['START_TIME'].min()).days
            if observation_days == 0:
                observation_days = 3  # é»˜è®¤3å¤©

            daily_orders = avg_frequency / observation_days
            annual_value = avg_order_value * daily_orders * 365

            # ç®€åŒ–LTVè®¡ç®—: LTV = å¹´ä»·å€¼ Ã— (1 / (1 - ç•™å­˜ç‡))
            ltv = annual_value * (1 / (1 - retention_rate))

            ltv_results.append({
                'ç”¨æˆ·åˆ†ç¾¤': segment,
                'ç”¨æˆ·æ•°é‡': user_count,
                'å æ¯”': f"{(user_count / total_users) * 100:.1f}%",
                'å¹³å‡éª‘è¡Œæ¬¡æ•°': round(avg_frequency, 2),
                'å‡è®¾ç•™å­˜ç‡': f"{retention_rate * 100:.0f}%",
                'é¢„ä¼°å¹´ä»·å€¼': round(annual_value, 2),
                'é¢„ä¼°LTV': round(ltv, 2)
            })

        return pd.DataFrame(ltv_results)


    # è®¡ç®—LTV
    ltv_analysis = estimate_customer_ltv_safe(rfm_analysis)

    if not ltv_analysis.empty:
        print("\n" + "=" * 60)
        print("ç”¨æˆ·ç”Ÿå‘½å‘¨æœŸä»·å€¼(LTV)åˆ†æ")
        print("=" * 60)
        print(ltv_analysis.to_string(index=False))

        # è®¡ç®—æ€»ä½“ç”¨æˆ·ä»·å€¼
        total_ltv = 0
        for _, row in ltv_analysis.iterrows():
            total_ltv += row['ç”¨æˆ·æ•°é‡'] * row['é¢„ä¼°LTV']

        print(f"\nå½“å‰ç”¨æˆ·æ€»ç”Ÿå‘½å‘¨æœŸä»·å€¼é¢„ä¼°: {total_ltv:,.2f}å…ƒ")

        # å¯è§†åŒ–LTVåˆ†æ
        fig, axes = plt.subplots(1, 2, figsize=(14, 6))

        # å„åˆ†ç¾¤LTVå¯¹æ¯”
        bars = axes[0].bar(ltv_analysis['ç”¨æˆ·åˆ†ç¾¤'], ltv_analysis['é¢„ä¼°LTV'],
                           color=['gold', 'lightblue', 'lightcoral'], alpha=0.7)
        axes[0].set_title('å„ç”¨æˆ·åˆ†ç¾¤ç”Ÿå‘½å‘¨æœŸä»·å€¼(LTV)', fontsize=12, fontweight='bold')
        axes[0].set_ylabel('LTV (å…ƒ)')

        # åœ¨æŸ±çŠ¶å›¾ä¸Šæ·»åŠ æ•°å€¼
        for bar in bars:
            height = bar.get_height()
            axes[0].text(bar.get_x() + bar.get_width() / 2., height + 5,
                         f'{height:.0f}å…ƒ', ha='center', va='bottom', fontweight='bold')

        # ç”¨æˆ·ä»·å€¼æ„æˆ
        ltv_analysis['æ€»ä»·å€¼'] = ltv_analysis['ç”¨æˆ·æ•°é‡'] * ltv_analysis['é¢„ä¼°LTV']
        axes[1].pie(ltv_analysis['æ€»ä»·å€¼'], labels=ltv_analysis['ç”¨æˆ·åˆ†ç¾¤'], autopct='%1.1f%%',
                    colors=['gold', 'lightblue', 'lightcoral'])
        axes[1].set_title('ç”¨æˆ·æ€»ä»·å€¼æ„æˆ', fontsize=12, fontweight='bold')

        plt.tight_layout()
        plt.show()
    else:
        print("LTVåˆ†æå¤±è´¥")
else:
    print("RFMåˆ†æå¤±è´¥ï¼Œè·³è¿‡ç”¨æˆ·ä»·å€¼åˆ†æ")


def generate_business_report(df, supply_demand_analysis=None, dispatch_plan=None,
                             impact_analysis=None, rfm_analysis=None, ltv_analysis=None):
    """
    ç”Ÿæˆå®Œæ•´çš„å•†ä¸šåˆ†ææŠ¥å‘Š - ä¿®å¤ç‰ˆ
    """
    print("\n" + "=" * 80)
    print("å•†ä¸šåˆ†ææŠ¥å‘Šæ‘˜è¦")
    print("=" * 80)

    # åŸºç¡€è¿è¥æŒ‡æ ‡
    total_orders = len(df)
    unique_users = df['USER_ID'].nunique() if 'USER_ID' in df.columns else 0
    total_revenue_estimate = total_orders * 2  # å‡è®¾æ¯å•2å…ƒ
    avg_orders_per_user = total_orders / unique_users if unique_users > 0 else 0

    print(f"\nğŸ“Š åŸºç¡€è¿è¥æŒ‡æ ‡:")
    print(f"   â€¢ æ€»è®¢å•é‡: {total_orders:,} å•")
    print(f"   â€¢ æœåŠ¡ç”¨æˆ·æ•°: {unique_users:,} äºº")
    print(f"   â€¢ å•ç”¨æˆ·å‡è®¢å•: {avg_orders_per_user:.2f} å•")
    print(f"   â€¢ é¢„ä¼°æ€»æ”¶å…¥: {total_revenue_estimate:,.0f} å…ƒ")

    # æ—¶é—´åˆ†æ - ä¿®å¤æ—¶é—´è®¡ç®—é—®é¢˜
    if 'START_TIME' in df.columns:
        try:
            # ç¡®ä¿æ˜¯datetimeç±»å‹
            if pd.api.types.is_datetime64_any_dtype(df['START_TIME']):
                date_range = df['START_TIME'].max() - df['START_TIME'].min()
                print(f"   â€¢ åˆ†ææ—¶é—´èŒƒå›´: {date_range.days} å¤©")
            else:
                # å¦‚æœä¸æ˜¯datetimeç±»å‹ï¼Œå°è¯•è½¬æ¢
                df_temp = df.copy()
                df_temp['START_TIME'] = pd.to_datetime(df_temp['START_TIME'], errors='coerce')
                if pd.api.types.is_datetime64_any_dtype(df_temp['START_TIME']):
                    date_range = df_temp['START_TIME'].max() - df_temp['START_TIME'].min()
                    print(f"   â€¢ åˆ†ææ—¶é—´èŒƒå›´: {date_range.days} å¤©")
                else:
                    print(f"   â€¢ åˆ†ææ—¶é—´èŒƒå›´: æ— æ³•è®¡ç®—")
        except Exception as e:
            print(f"   â€¢ åˆ†ææ—¶é—´èŒƒå›´: è®¡ç®—å¤±è´¥ ({str(e)})")

    # ä¾›éœ€åˆ†æç»“æœ
    if supply_demand_analysis is not None and not supply_demand_analysis.empty:
        # å®‰å…¨åœ°è·å–é—®é¢˜åŒºåŸŸæ•°é‡
        try:
            shortage_count = len(critical_shortage) if 'critical_shortage' in locals() else 0
            excess_count = len(critical_excess) if 'critical_excess' in locals() else 0

            print(f"\nğŸ” ä¾›éœ€ç“¶é¢ˆåˆ†æ:")
            print(f"   â€¢ è¯†åˆ«ä¸¥é‡çŸ­ç¼ºåŒºåŸŸ: {shortage_count} ä¸ª")
            print(f"   â€¢ è¯†åˆ«ä¸¥é‡è¿‡å‰©åŒºåŸŸ: {excess_count} ä¸ª")

            # ä¸»è¦é—®é¢˜æ—¶æ®µ
            if 'time_period' in supply_demand_analysis.columns:
                period_issues = supply_demand_analysis.groupby('time_period')['demand_supply_ratio'].mean()
                if len(period_issues) > 0:
                    worst_period = period_issues.idxmax()
                    print(f"   â€¢ æœ€ä¸¥é‡é—®é¢˜æ—¶æ®µ: {worst_period}")
        except Exception as e:
            print(f"   â€¢ ä¾›éœ€åˆ†æç»“æœæ˜¾ç¤ºå¤±è´¥: {str(e)}")

    # è°ƒåº¦ä¼˜åŒ–æ•ˆç›Š
    if dispatch_plan is not None and impact_analysis is not None:
        try:
            if not dispatch_plan.empty and not impact_analysis.empty:
                total_benefit = impact_analysis['net_benefit'].sum()
                total_cost = impact_analysis['dispatch_cost'].sum()
                total_roi = total_benefit / total_cost if total_cost > 0 else 0

                print(f"\nğŸ’¡ è°ƒåº¦ä¼˜åŒ–æ–¹æ¡ˆ:")
                print(f"   â€¢ å¯è¡Œè°ƒåº¦å»ºè®®: {len(dispatch_plan)} æ¡")
                print(f"   â€¢ æ€»å®æ–½æˆæœ¬: {total_cost:.0f} å…ƒ")
                print(f"   â€¢ é¢„è®¡å‡€æ”¶ç›Š: {total_benefit:.0f} å…ƒ")
                print(f"   â€¢ æŠ•èµ„å›æŠ¥ç‡: {total_roi:.1f} å€")
                print(f"   â€¢ é¢„è®¡æ–°å¢è®¢å•: {impact_analysis['additional_orders'].sum()} å•")
        except Exception as e:
            print(f"   â€¢ è°ƒåº¦ä¼˜åŒ–ç»“æœæ˜¾ç¤ºå¤±è´¥: {str(e)}")

    # ç”¨æˆ·ä»·å€¼æ´å¯Ÿ
    if rfm_analysis is not None and ltv_analysis is not None:
        try:
            if not rfm_analysis.empty and not ltv_analysis.empty:
                high_value_users = len(rfm_analysis[rfm_analysis['user_segment'] == 'é«˜ä»·å€¼ç”¨æˆ·'])
                high_value_ratio = (high_value_users / len(rfm_analysis)) * 100
                total_ltv_value = sum(ltv_analysis['ç”¨æˆ·æ•°é‡'] * ltv_analysis['é¢„ä¼°LTV'])

                print(f"\nğŸ‘¥ ç”¨æˆ·ä»·å€¼æ´å¯Ÿ:")
                print(f"   â€¢ é«˜ä»·å€¼ç”¨æˆ·å æ¯”: {high_value_ratio:.1f}%")
                print(f"   â€¢ ç”¨æˆ·æ€»ç”Ÿå‘½å‘¨æœŸä»·å€¼: {total_ltv_value:,.0f} å…ƒ")
                print(f"   â€¢ æœ€å…·ä»·å€¼ç”¨æˆ·ç‰¹å¾: é«˜é¢‘æ¬¡ã€é«˜é‡Œç¨‹ã€è¿‘æœŸæ´»è·ƒ")
        except Exception as e:
            print(f"   â€¢ ç”¨æˆ·ä»·å€¼åˆ†æç»“æœæ˜¾ç¤ºå¤±è´¥: {str(e)}")

    # æˆ˜ç•¥å»ºè®®
    print(f"\nğŸ¯ æ ¸å¿ƒæˆ˜ç•¥å»ºè®®:")

    has_dispatch = False
    has_rfm = False

    try:
        if dispatch_plan is not None and not dispatch_plan.empty:
            has_dispatch = True
            print(f"  1. ç«‹å³æ‰§è¡Œé«˜ROIè°ƒåº¦æ–¹æ¡ˆ")
            print(f"     â€¢ ä¼˜å…ˆå®æ–½å‰{min(5, len(dispatch_plan))}æ¡è°ƒåº¦å»ºè®®")
            if 'total_roi' in locals():
                print(f"     â€¢ é¢„è®¡{total_roi:.1f}å€æŠ•èµ„å›æŠ¥")
    except:
        pass

    try:
        if rfm_analysis is not None and not rfm_analysis.empty:
            has_rfm = True
            high_value_count = len(rfm_analysis[rfm_analysis['user_segment'] == 'é«˜ä»·å€¼ç”¨æˆ·'])
            print(f"  2. å¯åŠ¨é«˜ä»·å€¼ç”¨æˆ·ç»´æŠ¤è®¡åˆ’")
            print(f"     â€¢ é’ˆå¯¹{high_value_count}åé«˜ä»·å€¼ç”¨æˆ·")
            print(f"     â€¢ é¢„è®¡æå‡ç•™å­˜ç‡5-10%")
    except:
        pass

    if not has_dispatch:
        print(f"  1. ä¼˜åŒ–è½¦è¾†è°ƒåº¦ç­–ç•¥")
        print(f"     â€¢ åŸºäºæ•°æ®åˆ†æè¯†åˆ«ä¾›éœ€çƒ­ç‚¹")
        print(f"     â€¢ å»ºç«‹åŠ¨æ€è°ƒåº¦æœºåˆ¶")

    if not has_rfm:
        print(f"  2. å®æ–½ç”¨æˆ·åˆ†å±‚è¿è¥")
        print(f"     â€¢ è¯†åˆ«é«˜ä»·å€¼ç”¨æˆ·ç‰¹å¾")
        print(f"     â€¢ åˆ¶å®šå·®å¼‚åŒ–æœåŠ¡ç­–ç•¥")

    print(f"  3. å»ºç«‹é¢„æµ‹æ€§è°ƒåº¦ç³»ç»Ÿ")
    print(f"     â€¢ åŸºäºå†å²æ•°æ®çš„éœ€æ±‚é¢„æµ‹")
    print(f"     â€¢ è‡ªåŠ¨åŒ–è°ƒåº¦å†³ç­–æ”¯æŒ")

    print(f"  4. ä¼˜åŒ–è½¦è¾†æŠ•æ”¾ç­–ç•¥")
    print(f"     â€¢ é‡ç‚¹ä¿éšœçŸ­ç¼ºåŒºåŸŸä¾›ç»™")
    print(f"     â€¢ åŠ¨æ€è°ƒæ•´è½¦è¾†åˆ†å¸ƒ")

    print(f"\nğŸ“ˆ é¢„æœŸå•†ä¸šä»·å€¼:")
    estimated_improvement = total_revenue_estimate * 0.15  # é¢„è®¡æå‡15%
    print(f"   â€¢ é€šè¿‡ä¼˜åŒ–é¢„è®¡å¯æå‡æ”¶å…¥: {estimated_improvement:,.0f} å…ƒ")
    print(f"   â€¢ ç”¨æˆ·æ»¡æ„åº¦é¢„è®¡æå‡: 10-20%")
    print(f"   â€¢ è¿è¥æ•ˆç‡é¢„è®¡æå‡: 15-25%")


# å¯¼å‡ºå…³é”®ç»“æœ
def export_key_results():
    """å¯¼å‡ºå…³é”®åˆ†æç»“æœ"""
    import os

    # åˆ›å»ºç»“æœç›®å½•
    if not os.path.exists('analysis_results'):
        os.makedirs('analysis_results')

    # ä¿å­˜ä¾›éœ€åˆ†æç»“æœ
    if 'detailed_analysis' in locals() and not detailed_analysis.empty:
        detailed_analysis.to_csv('analysis_results/supply_demand_analysis.csv', index=False, encoding='utf-8-sig')
        print("âœ“ ä¾›éœ€åˆ†æç»“æœå·²ä¿å­˜")

    # ä¿å­˜è°ƒåº¦æ–¹æ¡ˆ
    if 'dispatch_plan' in locals() and not dispatch_plan.empty:
        dispatch_plan.to_csv('analysis_results/dispatch_recommendations.csv', index=False, encoding='utf-8-sig')
        print("âœ“ è°ƒåº¦å»ºè®®å·²ä¿å­˜")

    # ä¿å­˜ç”¨æˆ·åˆ†æ
    if 'rfm_analysis' in locals() and not rfm_analysis.empty:
        rfm_analysis.to_csv('analysis_results/user_rfm_analysis.csv', index=False, encoding='utf-8-sig')
        print("âœ“ ç”¨æˆ·RFMåˆ†æå·²ä¿å­˜")

    # ä¿å­˜LTVåˆ†æ
    if 'ltv_analysis' in locals() and not ltv_analysis.empty:
        ltv_analysis.to_csv('analysis_results/customer_ltv_analysis.csv', index=False, encoding='utf-8-sig')
        print("âœ“ ç”¨æˆ·LTVåˆ†æå·²ä¿å­˜")

    # ä¿å­˜è°ƒåº¦æ•ˆæœåˆ†æ
    if 'impact_analysis' in locals() and not impact_analysis.empty:
        impact_analysis.to_csv('analysis_results/dispatch_impact_analysis.csv', index=False, encoding='utf-8-sig')
        print("âœ“ è°ƒåº¦æ•ˆæœåˆ†æå·²ä¿å­˜")

    print("\næ‰€æœ‰åˆ†æç»“æœå·²å¯¼å‡ºè‡³ 'analysis_results' ç›®å½•")


# æ‰§è¡Œå¯¼å‡º
export_key_results()

# æœ€ç»ˆæ€»ç»“
print("\n" + "=" * 80)
print("é¡¹ç›®å®Œæˆæ€»ç»“")
print("=" * 80)
print("âœ… å·²å®Œæˆçš„åˆ†ææ¨¡å—:")
completed_modules = []

if 'detailed_analysis' in locals() and not detailed_analysis.empty:
    completed_modules.append("â€¢ æ—¶ç©ºä¾›éœ€æ·±åº¦åˆ†æ")
if 'dispatch_plan' in locals() and not dispatch_plan.empty:
    completed_modules.append("â€¢ æ™ºèƒ½è°ƒåº¦ä¼˜åŒ–ç®—æ³•")
if 'rfm_analysis' in locals() and not rfm_analysis.empty:
    completed_modules.append("â€¢ ç”¨æˆ·RFMä»·å€¼åˆ†ç¾¤")
if 'ltv_analysis' in locals() and not ltv_analysis.empty:
    completed_modules.append("â€¢ ç”Ÿå‘½å‘¨æœŸä»·å€¼é¢„æµ‹")

if completed_modules:
    for module in completed_modules:
        print(module)
else:
    print("   â€¢ åŸºç¡€æ•°æ®é¢„å¤„ç†ä¸è´¨é‡éªŒè¯")

print("\nğŸ“ˆ æ ¸å¿ƒç«äº‰åŠ›æå‡:")
print("   â€¢ å®Œæ•´çš„æ•°æ®åˆ†æé¡¹ç›®ç»éªŒ")
print("   â€¢ å•†ä¸šæ€ç»´ä¸ä¸šåŠ¡æ´å¯Ÿèƒ½åŠ›")
print("   â€¢ å¤æ‚é—®é¢˜å»ºæ¨¡ä¸è§£å†³èƒ½åŠ›")
print("   â€¢ ä»æ•°æ®åˆ°å†³ç­–çš„å®Œæ•´é—­ç¯")
print("   â€¢ å¯é‡åŒ–çš„å•†ä¸šä»·å€¼è¯æ˜")

print("\nğŸ¯ ä¸‹ä¸€æ­¥å»ºè®®:")
print("   â€¢ å°†åˆ†æç»“æœæ•´ç†åˆ°Power BIä»ªè¡¨æ¿")
print("   â€¢ å‡†å¤‡é¡¹ç›®æ¼”ç¤ºæ–‡ç¨¿å’Œé¢è¯•è¯æœ¯")
print("   â€¢ åœ¨GitHubä¸Šåˆ›å»ºé¡¹ç›®ä»“åº“å±•ç¤ºä»£ç ")
print("   â€¢ æ’°å†™æŠ€æœ¯åšå®¢æ€»ç»“é¡¹ç›®ç»éªŒ")

print("\nğŸ‰ ç¬¬äº”é˜¶æ®µåˆ†æå®Œæˆï¼")
print("=" * 80)


def force_export_all_results():
    """
    å¼ºåˆ¶å¯¼å‡ºæ‰€æœ‰åˆ†æç»“æœ - ç¡®ä¿ä¸€å®šæœ‰è¾“å‡º
    """
    import os
    import pandas as pd
    from datetime import datetime

    print("\n" + "=" * 60)
    print("å¼ºåˆ¶å¯¼å‡ºæ‰€æœ‰åˆ†æç»“æœ")
    print("=" * 60)

    # ç¡®ä¿ç›®å½•å­˜åœ¨
    results_dir = 'analysis_results'
    if not os.path.exists(results_dir):
        os.makedirs(results_dir)
        print(f"åˆ›å»ºç›®å½•: {results_dir}")

    files_created = []

    # 1. åŸºç¡€æ•°æ®ç»Ÿè®¡ (æ€»æ˜¯å¯ä»¥ç”Ÿæˆ)
    try:
        basic_stats = {
            'ç»Ÿè®¡æ—¶é—´': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'æ€»è®¢å•é‡': len(df_clean),
            'å”¯ä¸€ç”¨æˆ·æ•°': df_clean['USER_ID'].nunique() if 'USER_ID' in df_clean.columns else 'æœªçŸ¥',
            'æ€»éª‘è¡Œè·ç¦»_km': f"{df_clean['distance_km'].sum():.2f}" if 'distance_km' in df_clean.columns else 'æœªçŸ¥',
            'æ€»éª‘è¡Œæ—¶é•¿_åˆ†é’Ÿ': f"{df_clean['ride_duration'].sum():.2f}" if 'ride_duration' in df_clean.columns else 'æœªçŸ¥',
            'å¹³å‡éª‘è¡Œè·ç¦»_km': f"{df_clean['distance_km'].mean():.2f}" if 'distance_km' in df_clean.columns else 'æœªçŸ¥',
            'å¹³å‡éª‘è¡Œæ—¶é•¿_åˆ†é’Ÿ': f"{df_clean['ride_duration'].mean():.2f}" if 'ride_duration' in df_clean.columns else 'æœªçŸ¥'
        }

        basic_stats_df = pd.DataFrame([basic_stats])
        basic_stats_df.to_csv(f'{results_dir}/01_åŸºç¡€ç»Ÿè®¡æ•°æ®.csv', index=False, encoding='utf-8-sig')
        files_created.append('01_åŸºç¡€ç»Ÿè®¡æ•°æ®.csv')
        print("âœ“ åŸºç¡€ç»Ÿè®¡æ•°æ®å·²å¯¼å‡º")
    except Exception as e:
        print(f"âœ— åŸºç¡€ç»Ÿè®¡æ•°æ®å¯¼å‡ºå¤±è´¥: {e}")

    # 2. æ—¶é—´åˆ†å¸ƒåˆ†æ
    try:
        if 'hour' in df_clean.columns:
            hourly_data = df_clean['hour'].value_counts().sort_index().reset_index()
            hourly_data.columns = ['å°æ—¶', 'è®¢å•é‡']
            hourly_data.to_csv(f'{results_dir}/02_å°æ—¶è®¢å•åˆ†å¸ƒ.csv', index=False, encoding='utf-8-sig')
            files_created.append('02_å°æ—¶è®¢å•åˆ†å¸ƒ.csv')
            print("âœ“ æ—¶é—´åˆ†å¸ƒæ•°æ®å·²å¯¼å‡º")
    except Exception as e:
        print(f"âœ— æ—¶é—´åˆ†å¸ƒæ•°æ®å¯¼å‡ºå¤±è´¥: {e}")

    # 3. ç”¨æˆ·è¡Œä¸ºæ‘˜è¦
    try:
        if 'USER_ID' in df_clean.columns:
            user_behavior = df_clean.groupby('USER_ID').agg({
                'distance_km': ['count', 'sum', 'mean'],
                'ride_duration': ['sum', 'mean']
            }).round(2)

            # æ‰å¹³åŒ–åˆ—å
            user_behavior.columns = ['éª‘è¡Œæ¬¡æ•°', 'æ€»è·ç¦»_km', 'å¹³å‡è·ç¦»_km', 'æ€»æ—¶é•¿_åˆ†é’Ÿ', 'å¹³å‡æ—¶é•¿_åˆ†é’Ÿ']
            user_behavior = user_behavior.reset_index()
            user_behavior.to_csv(f'{results_dir}/03_ç”¨æˆ·è¡Œä¸ºæ‘˜è¦.csv', index=False, encoding='utf-8-sig')
            files_created.append('03_ç”¨æˆ·è¡Œä¸ºæ‘˜è¦.csv')
            print("âœ“ ç”¨æˆ·è¡Œä¸ºæ•°æ®å·²å¯¼å‡º")
    except Exception as e:
        print(f"âœ— ç”¨æˆ·è¡Œä¸ºæ•°æ®å¯¼å‡ºå¤±è´¥: {e}")

    # 4. åœ°ç†åˆ†å¸ƒç»Ÿè®¡
    try:
        if all(col in df_clean.columns for col in ['START_LAT', 'START_LNG']):
            # åˆ›å»ºåœ°ç†ç½‘æ ¼ç»Ÿè®¡
            grid_size = 0.01
            df_clean['grid_lat'] = (df_clean['START_LAT'] // grid_size) * grid_size
            df_clean['grid_lng'] = (df_clean['START_LNG'] // grid_size) * grid_size
            grid_stats = df_clean.groupby(['grid_lat', 'grid_lng']).size().reset_index(name='è®¢å•é‡')
            grid_stats.to_csv(f'{results_dir}/04_åœ°ç†åˆ†å¸ƒç»Ÿè®¡.csv', index=False, encoding='utf-8-sig')
            files_created.append('04_åœ°ç†åˆ†å¸ƒç»Ÿè®¡.csv')
            print("âœ“ åœ°ç†åˆ†å¸ƒæ•°æ®å·²å¯¼å‡º")
    except Exception as e:
        print(f"âœ— åœ°ç†åˆ†å¸ƒæ•°æ®å¯¼å‡ºå¤±è´¥: {e}")

    # 5. éª‘è¡Œè·ç¦»åˆ†å¸ƒ
    try:
        if 'distance_km' in df_clean.columns:
            distance_bins = [0, 1, 3, 5, 10, 20, 50, 100]
            distance_labels = ['0-1km', '1-3km', '3-5km', '5-10km', '10-20km', '20-50km', '50km+']
            df_clean['distance_range'] = pd.cut(df_clean['distance_km'], bins=distance_bins, labels=distance_labels)
            distance_dist = df_clean['distance_range'].value_counts().sort_index().reset_index()
            distance_dist.columns = ['è·ç¦»èŒƒå›´', 'è®¢å•é‡']
            distance_dist.to_csv(f'{results_dir}/05_éª‘è¡Œè·ç¦»åˆ†å¸ƒ.csv', index=False, encoding='utf-8-sig')
            files_created.append('05_éª‘è¡Œè·ç¦»åˆ†å¸ƒ.csv')
            print("âœ“ éª‘è¡Œè·ç¦»åˆ†å¸ƒå·²å¯¼å‡º")
    except Exception as e:
        print(f"âœ— éª‘è¡Œè·ç¦»åˆ†å¸ƒå¯¼å‡ºå¤±è´¥: {e}")

    # 6. å°è¯•å¯¼å‡ºé«˜çº§åˆ†æç»“æœï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    advanced_results = {
        'detailed_analysis': '06_ä¾›éœ€åˆ†æç»“æœ.csv',
        'dispatch_plan': '07_è°ƒåº¦å»ºè®®æ–¹æ¡ˆ.csv',
        'rfm_analysis': '08_ç”¨æˆ·RFMåˆ†ç¾¤.csv',
        'ltv_analysis': '09_ç”¨æˆ·LTVåˆ†æ.csv',
        'impact_analysis': '10_è°ƒåº¦æ•ˆæœæ¨¡æ‹Ÿ.csv'
    }

    for var_name, file_name in advanced_results.items():
        try:
            if var_name in globals():
                var_value = globals()[var_name]
                if isinstance(var_value, pd.DataFrame) and not var_value.empty:
                    var_value.to_csv(f'{results_dir}/{file_name}', index=False, encoding='utf-8-sig')
                    files_created.append(file_name)
                    print(f"âœ“ {file_name} å·²å¯¼å‡º")
        except Exception as e:
            print(f"âœ— {file_name} å¯¼å‡ºå¤±è´¥: {e}")

    # 7. åˆ›å»ºåˆ†ææŠ¥å‘Šæ‘˜è¦
    try:
        report_content = f"""æ•°æ®åˆ†ææŠ¥å‘Šæ‘˜è¦
ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

åˆ†ææ¦‚å†µ:
- æ€»è®¢å•é‡: {len(df_clean):,}
- å”¯ä¸€ç”¨æˆ·æ•°: {df_clean['USER_ID'].nunique() if 'USER_ID' in df_clean.columns else 'æœªçŸ¥':,}
- åˆ†ææ–‡ä»¶æ•°: {len(files_created)}

ç”Ÿæˆçš„æ–‡ä»¶:
"""
        for file in files_created:
            file_path = os.path.join(results_dir, file)
            if os.path.exists(file_path):
                file_size = os.path.getsize(file_path)
                report_content += f"- {file} ({file_size} bytes)\n"
            else:
                report_content += f"- {file} (æ–‡ä»¶ç¼ºå¤±)\n"

        with open(f'{results_dir}/README_æŠ¥å‘Šè¯´æ˜.txt', 'w', encoding='utf-8') as f:
            f.write(report_content)
        files_created.append('README_æŠ¥å‘Šè¯´æ˜.txt')
        print("âœ“ æŠ¥å‘Šè¯´æ˜æ–‡ä»¶å·²åˆ›å»º")
    except Exception as e:
        print(f"âœ— æŠ¥å‘Šè¯´æ˜æ–‡ä»¶åˆ›å»ºå¤±è´¥: {e}")

    # æœ€ç»ˆæ£€æŸ¥
    print(f"\nå¯¼å‡ºå®Œæˆç»Ÿè®¡:")
    print(f"- å°è¯•å¯¼å‡ºæ–‡ä»¶: {len(files_created)} ä¸ª")

    actual_files = os.listdir(results_dir)
    print(f"- å®é™…ç”Ÿæˆæ–‡ä»¶: {len(actual_files)} ä¸ª")

    if actual_files:
        print("\nç”Ÿæˆçš„æ–‡ä»¶åˆ—è¡¨:")
        for file in actual_files:
            file_path = os.path.join(results_dir, file)
            file_size = os.path.getsize(file_path)
            print(f"  â€¢ {file} ({file_size} bytes)")
    else:
        print("\nâŒ ä¸¥é‡é”™è¯¯: ç›®å½•ä»ç„¶ä¸ºç©º!")
        print("å¯èƒ½çš„åŸå› :")
        print("1. ç›®å½•æƒé™é—®é¢˜")
        print("2. ç£ç›˜ç©ºé—´ä¸è¶³")
        print("3. é˜²ç—…æ¯’è½¯ä»¶é˜»æ­¢")
        print("4. æ–‡ä»¶ç³»ç»Ÿé”™è¯¯")

        # å°è¯•åœ¨å…¶ä»–ä½ç½®åˆ›å»º
        alternative_dir = 'my_analysis_results'
        if not os.path.exists(alternative_dir):
            os.makedirs(alternative_dir)
            test_file = os.path.join(alternative_dir, 'test.txt')
            with open(test_file, 'w') as f:
                f.write("æµ‹è¯•æ–‡ä»¶")
            print(f"\nå·²åœ¨å¤‡ç”¨ç›®å½• {alternative_dir} åˆ›å»ºæµ‹è¯•æ–‡ä»¶")

    return files_created


# åœ¨ä»£ç æœ€åè°ƒç”¨å¼ºåˆ¶å¯¼å‡º
print("\nå¼€å§‹å¼ºåˆ¶å¯¼å‡ºæ‰€æœ‰ç»“æœ...")
exported_files = force_export_all_results()

if exported_files:
    print(f"\nğŸ‰ æˆåŠŸå¯¼å‡º {len(exported_files)} ä¸ªæ–‡ä»¶!")
    print("è¯·æ£€æŸ¥ 'analysis_results' ç›®å½•")
else:
    print("\nâŒ å¯¼å‡ºå¤±è´¥ï¼Œå°è¯•è¯Šæ–­é—®é¢˜...")

    # è¯Šæ–­é—®é¢˜
    import os

    current_dir = os.getcwd()
    target_dir = os.path.join(current_dir, 'analysis_results')

    print(f"å½“å‰å·¥ä½œç›®å½•: {current_dir}")
    print(f"ç›®æ ‡ç›®å½•: {target_dir}")
    print(f"ç›®æ ‡ç›®å½•å­˜åœ¨: {os.path.exists(target_dir)}")
    print(f"ç›®æ ‡ç›®å½•å¯å†™: {os.access(target_dir, os.W_OK) if os.path.exists(target_dir) else 'N/A'}")

    # å°è¯•ç›´æ¥å†™å…¥å½“å‰ç›®å½•
    try:
        test_file = 'test_direct_write.csv'
        pd.DataFrame({'test': [1, 2, 3]}).to_csv(test_file, index=False)
        if os.path.exists(test_file):
            os.remove(test_file)
            print("âœ“ å½“å‰ç›®å½•å†™å…¥æµ‹è¯•: é€šè¿‡")
        else:
            print("âœ— å½“å‰ç›®å½•å†™å…¥æµ‹è¯•: å¤±è´¥")
    except Exception as e:
        print(f"âœ— å½“å‰ç›®å½•å†™å…¥æµ‹è¯•: {e}")